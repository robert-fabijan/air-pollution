logsBucket: "gs://builds-logs-bucket"
steps:
  # - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  #   args:
  #   - gcloud
  #   - functions
  #   - deploy
  #   - function-air-pollution
  #   - --entry-point=gcloud_get_openweather_data_function
  #   - --region=us-central1
  #   - --source=./gcloud/gcloud_functions
  #   - --trigger-http
  #   - --runtime=python311

  # [START composer_cicd_dagsync_yaml]

  # # Step to download the Rust installation script
  # - name: 'gcr.io/cloud-builders/curl'
  #   args: ['--proto', '=https', '--tlsv1.2', '-sSf', '-o', '/tmp/rustup.sh', 'https://sh.rustup.rs']

  # # Step to execute the Rust installation script
  # - name: 'ubuntu'
  #   entrypoint: 'bash'
  #   args: ['/tmp/rustup.sh', '-y']
    

  # Step to install Airflow and other Python dependencies
  # build the docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/${PROJECT_ID}/cicd:${SHORT_SHA}', '.'] # tag docker image with commit sha
    id: 'docker build'

  # # run the dag tests
  # - name: 'gcr.io/cloud-builders/docker'
  #   args: ['run', 'gcr.io/${PROJECT_ID}/cicd:${SHORT_SHA}']
  #   id: 'docker run'
    

  # # Run your Python script to move dags to folder
  # - name: 'python'
  #   entrypoint: 'python'
  #   args: ['gcloud/gcloud_data_pipeline/upload_dags_to_composer.py', '--dags_directory=dags/', '--dags_bucket=gs://air-pollution-bucket/dags']

  # Other method of running dags to your folder
  # - name: 'gcr.io/cloud-builders/gsutil'
  #   args:
  #     - '-m'
  #     - '-rsync'
  #     - '-d'
  #     - '-r'
  #     - 'gcloud/gcloud_data_pipeline/dags/'
  #     - 'gs://air-pollution-bucket/dags2'
    